{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "651d911e",
   "metadata": {},
   "source": [
    "## Import all libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "172e18bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import openpyxl\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc62477",
   "metadata": {},
   "source": [
    "### Open .xlsx files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2e32d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_1 = \"../files/dataset_30k.xlsx\"\n",
    "def load_dataset(file_path):\n",
    "    return pd.read_excel(file_path)\n",
    "\n",
    "dataset = load_dataset(file_path_1)\n",
    "\n",
    "# print(load_dataset(file_path_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "267cb970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss percent: 5.000277777777778%\n"
     ]
    }
   ],
   "source": [
    "def remove_n_percent_by_blocks(dataset, loss_percent, blocks=[(2, 2),(3, 2),(3, 1), (1, 3), (2, 1), (1, 1)]):\n",
    "    data = dataset.copy()\n",
    "    n_rows, n_cols = data.shape\n",
    "    total_elements = n_rows * n_cols\n",
    "    elements_to_remove = int(total_elements * loss_percent / 100)\n",
    "    removed = 0\n",
    "    while removed < elements_to_remove:\n",
    "        block = blocks[np.random.randint(len(blocks))]\n",
    "        h, w = block\n",
    "        i = np.random.randint(0, n_rows - h + 1)\n",
    "        j = np.random.randint(0, n_cols - w + 1)\n",
    "        block_data = data.iloc[i:i+h, j:j+w].isna()\n",
    "        new_missing = block_data.sum().sum()\n",
    "        data.iloc[i:i+h, j:j+w] = np.nan\n",
    "        removed += (h * w) - new_missing\n",
    "        \n",
    "    return data\n",
    "\n",
    "def count_loss_percent(dataset):\n",
    "    total_elements = dataset.size\n",
    "    missing_elements = dataset.isna().sum().sum()\n",
    "    return (missing_elements / total_elements) * 100\n",
    "\n",
    "def save_current_state(data: pd.DataFrame, file_path: str):\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "\n",
    "    with pd.ExcelWriter(file_path, engine='openpyxl') as writer:\n",
    "        data.to_excel(writer, index=False)\n",
    "        worksheet = writer.sheets['Sheet1']\n",
    "        for column in worksheet.columns:\n",
    "            max_length = 0\n",
    "            column_letter = column[0].column_letter\n",
    "\n",
    "            for cell in column:\n",
    "                try:\n",
    "                    if len(str(cell.value)) > max_length:\n",
    "                        max_length = len(str(cell.value))\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            adjusted_width = (max_length + 2) * 1.1\n",
    "            worksheet.column_dimensions[column_letter].width = adjusted_width\n",
    "\n",
    "dataset_loss_5 = remove_n_percent_by_blocks(dataset, 5)\n",
    "loss_percent_5 = count_loss_percent(dataset_loss_5)\n",
    "save_current_state(dataset_loss_5, \"dataset_loss_5.xlsx\")\n",
    "print(f\"Loss percent: {loss_percent_5}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc902e2e",
   "metadata": {},
   "source": [
    "### Mean Inputation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d09d5449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее значение по картам в 'Карта оплаты': 4131175007972241\n",
      "Средняя дата для 'Дата готовности анализов': 2020-07-04 11:10:16.923726848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_14328\\3651725267.py:199: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  mean_ts = valid.view(\"int64\").mean()\n",
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_14328\\3651725267.py:199: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  mean_ts = valid.view(\"int64\").mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Средняя дата для 'Дата посещения врача': 2020-07-02 10:16:37.461551872\n",
      "Столбец 'Симптомы': 194 уникальных элементов\n",
      "Столбец 'Симптомы': среднее id=98.451, выбраны id [np.int64(97), np.int64(98), np.int64(99)]\n",
      "Столбец 'Анализы': 101 уникальных элементов\n",
      "Столбец 'Анализы': среднее id=50.538, выбраны id [np.int64(50), np.int64(51)]\n",
      "Столбец 'Врач': 21 уникальных элементов\n",
      "Столбец 'Врач': среднее id=10.031, выбраны id [np.int64(10)]\n",
      "Столбец 'Фамилия': 631 уникальных элементов\n",
      "Столбец 'Фамилия': среднее id=317.197, выбраны id [np.int64(317)]\n",
      "Столбец 'Имя': 382 уникальных элементов\n",
      "Столбец 'Имя': среднее id=192.253, выбраны id [np.int64(192)]\n",
      "Столбец 'Отчество': 378 уникальных элементов\n",
      "Столбец 'Отчество': среднее id=190.672, выбраны id [np.int64(191)]\n",
      "Обрабатываемые паспортные столбцы: ['Паспортные данные']\n",
      "Паттерн маскирования: series_only\n",
      "Среднее значение замаскированных данных в 'Паспортные данные': 5497.21\n",
      "Заполнено 1669 пропусков в столбце 'Паспортные данные'\n"
     ]
    }
   ],
   "source": [
    "def delete_columns(data: pd.DataFrame, columns: list) -> pd.DataFrame:\n",
    "    return data.drop(columns=columns, axis=1)\n",
    "\n",
    "def passport_partial_mask_calculation(df, passport_columns=None, mask_pattern='last_4'):\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    def extract_numeric_part(passport_number, pattern):\n",
    "        if pd.isna(passport_number):\n",
    "            return np.nan\n",
    "        \n",
    "        cleaned = re.sub(r'[^\\d]', '', str(passport_number))\n",
    "        \n",
    "        if len(cleaned) != 10 or not cleaned.isdigit():\n",
    "            return np.nan\n",
    "        \n",
    "        if pattern == 'last_4':\n",
    "            return int(cleaned[6:])\n",
    "        elif pattern == 'first_6':\n",
    "            return int(cleaned[:6])\n",
    "        elif pattern == 'series_only':\n",
    "            return int(cleaned[:4])\n",
    "        elif pattern == 'number_only':\n",
    "            return int(cleaned[4:])\n",
    "        else:\n",
    "            raise ValueError(\"Неизвестный паттерн маскирования\")\n",
    "    \n",
    "    def fill_with_masked_mean(df, column, pattern):\n",
    "        numeric_values = df[column].apply(lambda x: extract_numeric_part(x, pattern))\n",
    "        \n",
    "        valid_values = numeric_values.dropna()\n",
    "        \n",
    "        if len(valid_values) == 0:\n",
    "            print(f\"В столбце '{column}' нет валидных паспортных данных\")\n",
    "            return df\n",
    "        \n",
    "        mean_value = valid_values.mean()\n",
    "        print(f\"Среднее значение замаскированных данных в '{column}': {mean_value:.2f}\")\n",
    "        \n",
    "        missing_mask = df[column].isnull()\n",
    "        missing_count = missing_mask.sum()\n",
    "        \n",
    "        if missing_count > 0:\n",
    "            existing_passports = set(df[column].dropna())\n",
    "            \n",
    "            for idx in df[missing_mask].index:\n",
    "                new_passport = generate_passport_from_masked_mean(mean_value, pattern, existing_passports)\n",
    "                df.at[idx, column] = new_passport\n",
    "                existing_passports.add(new_passport)\n",
    "            \n",
    "            print(f\"Заполнено {missing_count} пропусков в столбце '{column}'\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def generate_passport_from_masked_mean(mean_value, pattern, existing_passports):\n",
    "        while True:\n",
    "            if pattern == 'last_4':\n",
    "                base_number = int(mean_value) % 10000\n",
    "                last_4 = f\"{base_number:04d}\"\n",
    "                first_6 = f\"{random.randint(1, 99):02d}{random.randint(0, 99):02d}{random.randint(0, 99):02d}\"\n",
    "                passport = first_6 + last_4\n",
    "                \n",
    "            elif pattern == 'first_6':\n",
    "                base_number = max(10101, min(999999, int(mean_value)))\n",
    "                first_6 = f\"{base_number:06d}\"\n",
    "                last_4 = f\"{random.randint(0, 9999):04d}\"\n",
    "                passport = first_6 + last_4\n",
    "                \n",
    "            elif pattern == 'series_only':\n",
    "                series = max(101, min(9999, int(mean_value)))\n",
    "                series_str = f\"{series:04d}\"\n",
    "                number = f\"{random.randint(1, 999999):06d}\"\n",
    "                passport = series_str + number\n",
    "                \n",
    "            elif pattern == 'number_only':\n",
    "                number = max(1, min(999999, int(mean_value)))\n",
    "                number_str = f\"{number:06d}\"\n",
    "                series = f\"{random.randint(1, 99):02d}{random.randint(0, 99):02d}\"\n",
    "                passport = series + number_str\n",
    "            \n",
    "            formatted = f\"{passport[:2]} {passport[2:4]} {passport[4:]}\"\n",
    "            if formatted not in existing_passports:\n",
    "                return formatted\n",
    "    \n",
    "    if passport_columns is None:\n",
    "        passport_columns = []\n",
    "        for column in result_df.columns:\n",
    "            sample_data = result_df[column].dropna().head(5)\n",
    "            if len(sample_data) > 0:\n",
    "                if any(re.match(r'\\d{2} \\d{2} \\d{6}', str(x)) for x in sample_data):\n",
    "                    passport_columns.append(column)\n",
    "    \n",
    "    print(f\"Обрабатываемые паспортные столбцы: {passport_columns}\")\n",
    "    print(f\"Паттерн маскирования: {mask_pattern}\")\n",
    "    \n",
    "    for column in passport_columns:\n",
    "        if column in result_df.columns:\n",
    "            result_df = fill_with_masked_mean(result_df, column, mask_pattern)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def generalize_snils(df: pd.DataFrame, snils_column: str) -> pd.DataFrame:\n",
    "    if snils_column not in df.columns:\n",
    "        raise KeyError(f\"Column '{snils_column}' not found in dataset\")\n",
    "\n",
    "    result = df.copy()\n",
    "\n",
    "    result[snils_column] = result[snils_column].apply(\n",
    "        lambda x: 'Гражданин РФ' if x not in ['Гражданин РБ', 'Гражданин РК'] else x\n",
    "    )\n",
    "\n",
    "    col_series = result[snils_column].fillna('').astype(str)\n",
    "    kz_rows = int(col_series.str.contains('Гражданин РК', na=False).sum())\n",
    "    by_rows = int(col_series.str.contains('Гражданин РБ', na=False).sum())\n",
    "    ru_rows = int(col_series.str.contains('Гражданин РФ', na=False).sum())\n",
    "\n",
    "    counts = {\n",
    "        'Гражданин РФ': ru_rows,\n",
    "        'Гражданин РБ': by_rows,\n",
    "        'Гражданин РК': kz_rows,\n",
    "    }\n",
    "    most_popular = max(counts, key=counts.get)\n",
    "\n",
    "    result[snils_column] = result[snils_column].replace(\"\", np.nan)\n",
    "    result[snils_column] = result[snils_column].fillna(most_popular)\n",
    "\n",
    "    return result\n",
    "\n",
    "def fill_card_with_mean(df: pd.DataFrame, card_column: str) -> pd.DataFrame:\n",
    "    result = df.copy()\n",
    "    col = result[card_column].astype(str)\n",
    "\n",
    "    numeric = col.replace(\"nan\", np.nan).apply(\n",
    "        lambda x: re.sub(r\"[^\\d]\", \"\", str(x)) if pd.notna(x) else x\n",
    "    )\n",
    "\n",
    "    lengths = numeric.dropna().map(len)\n",
    "    if lengths.empty:\n",
    "        print(f\"В столбце '{card_column}' нет валидных номеров карт\")\n",
    "        return result\n",
    "\n",
    "    target_len = int(lengths.mode().iloc[0])\n",
    "\n",
    "    valid = numeric[numeric.map(lambda x: len(str(x)) == target_len if pd.notna(x) else False)]\n",
    "    if valid.empty:\n",
    "        print(f\"В столбце '{card_column}' нет номеров длиной {target_len}\")\n",
    "        return result\n",
    "\n",
    "    mean_value = int(valid.astype(int).mean().round())\n",
    "    print(f\"Среднее значение по картам в '{card_column}': {mean_value}\")\n",
    "\n",
    "    def mask_pattern(s: str) -> str:\n",
    "        return re.sub(r\"\\d\", \"X\", s)\n",
    "\n",
    "    non_null_raw = result[card_column].dropna().astype(str)\n",
    "    if non_null_raw.empty:\n",
    "        pattern = \"XXXXXXXXXXXXXXXX\"\n",
    "    else:\n",
    "        patterns = non_null_raw.apply(mask_pattern)\n",
    "        pattern = patterns.mode().iloc[0]\n",
    "\n",
    "    def format_with_pattern(number: int, pattern: str) -> str:\n",
    "        digits = list(str(number))\n",
    "        if len(digits) > target_len:\n",
    "            digits = digits[-target_len:]\n",
    "        elif len(digits) < target_len:\n",
    "            digits = list(\"0\" * (target_len - len(digits)) + \"\".join(digits))\n",
    "\n",
    "        result_chars = []\n",
    "        digit_idx = 0\n",
    "        for ch in pattern:\n",
    "            if ch.isdigit() or ch == \"X\":\n",
    "                if digit_idx < len(digits):\n",
    "                    result_chars.append(digits[digit_idx])\n",
    "                    digit_idx += 1\n",
    "                else:\n",
    "                    result_chars.append(\"0\")\n",
    "            else:\n",
    "                result_chars.append(ch)\n",
    "        return \"\".join(result_chars)\n",
    "\n",
    "    fill_value = format_with_pattern(mean_value, pattern)\n",
    "\n",
    "    mask = result[card_column].isna()\n",
    "    result.loc[mask, card_column] = fill_value\n",
    "\n",
    "    return result\n",
    "\n",
    "def fill_date_with_mean(df: pd.DataFrame, column: str, fmt: str = \"%Y-%m-%dT%H:%M%z\") -> pd.DataFrame:\n",
    "    result = df.copy()\n",
    "    result[column] = result[column].replace(\"\", np.nan)\n",
    "\n",
    "    dt = pd.to_datetime(result[column], errors=\"coerce\", format=fmt)\n",
    "\n",
    "    valid = dt.dropna()\n",
    "    if len(valid) == 0:\n",
    "        print(f\"В столбце '{column}' нет валидных дат\")\n",
    "        return result\n",
    "\n",
    "    mean_ts = valid.view(\"int64\").mean()\n",
    "    mean_dt = pd.to_datetime(int(mean_ts))\n",
    "\n",
    "    print(f\"Средняя дата для '{column}': {mean_dt}\")\n",
    "\n",
    "    if mean_dt.tz is not None:\n",
    "        mean_dt = mean_dt.tz_localize(None)\n",
    "    \n",
    "    formatted_date = mean_dt.strftime(\"%Y-%m-%dT%H:%M+03:00\")\n",
    "\n",
    "    mask = result[column].isna()\n",
    "    result.loc[mask, column] = formatted_date\n",
    "\n",
    "    return result\n",
    "\n",
    "def fill_with_mean(dataset, columns):\n",
    "    return dataset.fillna(dataset[columns].mean().round(-2))\n",
    "\n",
    "def impute_text_list_with_mean_item(\n",
    "    df: pd.DataFrame,\n",
    "    column: str,\n",
    "    sep: str = \",\",\n",
    "    k: int = 3,\n",
    ") -> pd.DataFrame:\n",
    "    result = df.copy()\n",
    "    result[column] = result[column].replace(\"\", np.nan)\n",
    "\n",
    "    all_items = (\n",
    "        result[column]\n",
    "        .dropna()\n",
    "        .astype(str)\n",
    "        .str.split(sep)\n",
    "        .explode()\n",
    "        .str.strip()\n",
    "    )\n",
    "    all_items = all_items[all_items != \"\"]\n",
    "    unique_items = all_items.unique()\n",
    "\n",
    "    if len(unique_items) == 0:\n",
    "        print(f\"Столбец '{column}': нет данных для импутации\")\n",
    "        return result\n",
    "\n",
    "    unique_items_sorted = sorted(unique_items)\n",
    "    item2id = {item: i + 1 for i, item in enumerate(unique_items_sorted)}\n",
    "    id2item = {v: k for k, v in item2id.items()}\n",
    "    print(f\"Столбец '{column}': {len(item2id)} уникальных элементов\")\n",
    "\n",
    "    def row_mean_id(text):\n",
    "        if pd.isna(text):\n",
    "            return np.nan\n",
    "        parts = [p.strip() for p in str(text).split(sep)]\n",
    "        ids = [item2id[p] for p in parts if p in item2id]\n",
    "        if not ids:\n",
    "            return np.nan\n",
    "        return float(np.mean(ids))\n",
    "\n",
    "    mean_ids = result[column].apply(row_mean_id)\n",
    "\n",
    "    col_mean = mean_ids.mean(skipna=True)\n",
    "    if np.isnan(col_mean):\n",
    "        print(f\"Столбец '{column}': не удалось вычислить среднее id\")\n",
    "        return result\n",
    "\n",
    "    all_ids = np.arange(1, len(id2item) + 1)\n",
    "    distances = np.abs(all_ids - col_mean)\n",
    "    nearest_indices = np.argsort(np.stack([distances, all_ids], axis=1), axis=0)[:, 0][:k]\n",
    "    chosen_ids = sorted(all_ids[nearest_indices])\n",
    "    replacement_items = [id2item[i] for i in chosen_ids]\n",
    "\n",
    "    replacement_str = f\"{sep} \".join(replacement_items)\n",
    "\n",
    "    print(f\"Столбец '{column}': среднее id={col_mean:.3f}, выбраны id {chosen_ids}\")\n",
    "\n",
    "    mask = result[column].isna()\n",
    "    result.loc[mask, column] = replacement_str\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def restore_dataset_mean_impute(dataset):\n",
    "    restored_dataset = dataset.copy()\n",
    "    restored_dataset = fill_with_mean(restored_dataset, [\"Стоимость анализов\"])\n",
    "    restored_dataset = generalize_snils(restored_dataset, \"СНИЛС\")\n",
    "    restored_dataset = fill_card_with_mean(restored_dataset, \"Карта оплаты\")\n",
    "    restored_dataset = fill_date_with_mean(restored_dataset, \"Дата готовности анализов\")\n",
    "    restored_dataset = fill_date_with_mean(restored_dataset, \"Дата посещения врача\")\n",
    "    restored_dataset = impute_text_list_with_mean_item(restored_dataset, \"Симптомы\")\n",
    "    restored_dataset = impute_text_list_with_mean_item(restored_dataset, \"Анализы\", k=2)\n",
    "    restored_dataset = impute_text_list_with_mean_item(restored_dataset, \"Врач\", k=1)\n",
    "\n",
    "    restored_dataset = impute_text_list_with_mean_item(restored_dataset, \"Фамилия\", k=1)\n",
    "    restored_dataset = impute_text_list_with_mean_item(restored_dataset, \"Имя\", k=1)\n",
    "    restored_dataset = impute_text_list_with_mean_item(restored_dataset, \"Отчество\", k=1)\n",
    "    restored_dataset[\"Паспортные данные\"] = restored_dataset[\"Паспортные данные\"].replace(\"\", np.nan)\n",
    "    restored_dataset = passport_partial_mask_calculation(\n",
    "        restored_dataset,\n",
    "        [\"Паспортные данные\"],\n",
    "        \"series_only\"\n",
    "    )\n",
    "    return restored_dataset\n",
    "\n",
    "restored_with_mean_dataset = restore_dataset_mean_impute(dataset_loss_5)\n",
    "save_current_state(restored_with_mean_dataset, \"dataset_loss_5_recovered_with_mean.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b221562",
   "metadata": {},
   "source": [
    "### Linear Regression imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b783bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Фамилия': линейная регрессия восстановила 957 пропусков\n",
      "'Имя': линейная регрессия восстановила 1463 пропусков\n",
      "'Отчество': линейная регрессия восстановила 1610 пропусков\n",
      "'Паспортные данные': линейная регрессия восстановила 1669 пропусков\n",
      "'СНИЛС': линейная регрессия восстановила 1589 пропусков\n",
      "'Симптомы': линейная регрессия восстановила 1643 пропусков\n",
      "'Врач': линейная регрессия восстановила 1602 пропусков\n",
      "'Дата посещения врача': линейная регрессия восстановила 1715 пропусков\n",
      "'Анализы': линейная регрессия восстановила 1637 пропусков\n",
      "'Дата готовности анализов': линейная регрессия восстановила 1602 пропусков\n",
      "'Стоимость анализов': линейная регрессия восстановила 1498 пропусков\n",
      "'Карта оплаты': линейная регрессия восстановила 1016 пропусков\n",
      "Целевая колонка: Стоимость анализов\n",
      "Свободный член w0: 5507.39168272462\n",
      "Коэффициенты w_n по признакам:\n",
      "  Врач                           -> -123.51600001195281\n",
      "  Дата готовности анализов       -> 2.92283232431694\n",
      "  Дата посещения врача           -> -2.0812183058552147\n",
      "  Анализы                        -> 0.19000354359056135\n",
      "  Отчество                       -> 0.14883165001819706\n",
      "  Имя                            -> 0.11547143052038937\n",
      "  Фамилия                        -> 0.016852279051991093\n",
      "  Карта оплаты                   -> -0.0068829671809618395\n",
      "  Паспортные данные              -> 0.0016900468371363192\n",
      "  Симптомы                       -> -0.0012246937198037433\n",
      "  СНИЛС                          -> 0.0008608660079087831\n",
      "[('Фамилия', np.float64(0.016852279051991093)), ('Имя', np.float64(0.11547143052038937)), ('Отчество', np.float64(0.14883165001819706)), ('Паспортные данные', np.float64(0.0016900468371363192)), ('СНИЛС', np.float64(0.0008608660079087831)), ('Симптомы', np.float64(-0.0012246937198037433)), ('Врач', np.float64(-123.51600001195281)), ('Дата посещения врача', np.float64(-2.0812183058552147)), ('Анализы', np.float64(0.19000354359056135)), ('Дата готовности анализов', np.float64(2.92283232431694)), ('Карта оплаты', np.float64(-0.0068829671809618395))]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "def build_column_encoder(col: pd.Series) -> dict:\n",
    "    \"\"\"\n",
    "    Строит простой энкодер для столбца: \n",
    "    - числовые оставляем как есть,\n",
    "    - категориальные/строки -> словарь value -> id (1..N).\n",
    "    \"\"\"\n",
    "    if pd.api.types.is_numeric_dtype(col):\n",
    "        return {\"type\": \"numeric\"}\n",
    "    # категориальный/текстовый\n",
    "    values = col.dropna().astype(str).unique()\n",
    "    mapping = {v: i + 1 for i, v in enumerate(sorted(values))}\n",
    "    return {\"type\": \"categorical\", \"mapping\": mapping}\n",
    "\n",
    "def encode_column(col: pd.Series, enc: dict) -> pd.Series:\n",
    "    if enc[\"type\"] == \"numeric\":\n",
    "        return pd.to_numeric(col, errors=\"coerce\")\n",
    "    mapping = enc[\"mapping\"]\n",
    "    return col.astype(str).map(mapping).astype(float)\n",
    "\n",
    "def decode_column(encoded: pd.Series, enc: dict, orig_dtype) -> pd.Series:\n",
    "    if enc[\"type\"] == \"numeric\":\n",
    "        return encoded.astype(orig_dtype)\n",
    "    mapping = enc[\"mapping\"]\n",
    "    inv_mapping = {v: k for k, v in mapping.items()}\n",
    "    # для пропусков/непопавших значений оставляем NaN\n",
    "    def _decode(v):\n",
    "        if pd.isna(v):\n",
    "            return np.nan\n",
    "        # берём ближайший id\n",
    "        v_int = int(round(float(v)))\n",
    "        return inv_mapping.get(v_int, np.nan)\n",
    "    decoded = encoded.map(_decode)\n",
    "    return decoded.astype(orig_dtype)\n",
    "\n",
    "def impute_column_by_regression(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str,\n",
    "    encoders: dict,\n",
    "    restored_mask: pd.DataFrame | None = None,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame | None]:\n",
    "    \"\"\"\n",
    "    Восстанавливает пропуски в ОДНОМ столбце target_col с помощью линейной регрессии,\n",
    "    используя только damaged_df. Разрешает пропуски в признаках: они заменяются на 0.\n",
    "    Если передан restored_mask, в нём помечаются восстановленные ячейки.\n",
    "    \"\"\"\n",
    "    result = df.copy()\n",
    "\n",
    "    if target_col not in result.columns:\n",
    "        raise KeyError(f\"Колонка '{target_col}' не найдена\")\n",
    "\n",
    "    target_enc = encoders[target_col]\n",
    "    target_encoded = encode_column(result[target_col], target_enc)\n",
    "\n",
    "    feature_cols = [c for c in result.columns if c != target_col]\n",
    "\n",
    "    X_encoded = pd.DataFrame(index=result.index)\n",
    "    for col in feature_cols:\n",
    "        X_encoded[col] = encode_column(result[col], encoders[col])\n",
    "\n",
    "    mask_known = target_encoded.notna()\n",
    "    mask_unknown = ~mask_known\n",
    "\n",
    "    if mask_known.sum() == 0 or mask_unknown.sum() == 0:\n",
    "        print(f\"'{target_col}': недостаточно данных для обучения/предсказания\")\n",
    "        return result, restored_mask\n",
    "\n",
    "    X_train = X_encoded.loc[mask_known, :]\n",
    "    y_train = target_encoded.loc[mask_known]\n",
    "\n",
    "    X_pred = X_encoded.loc[mask_unknown, :]\n",
    "\n",
    "    X_train = X_train.fillna(0)\n",
    "    X_pred = X_pred.fillna(0)\n",
    "\n",
    "    if len(X_train) == 0 or len(X_pred) == 0:\n",
    "        print(f\"'{target_col}': нет валидных строк после подготовки данных\")\n",
    "        return result, restored_mask\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train.values, y_train.values)\n",
    "\n",
    "    y_pred = model.predict(X_pred.values)\n",
    "\n",
    "    # вставляем предсказания\n",
    "    target_encoded.loc[mask_unknown] = y_pred\n",
    "\n",
    "    restored_col = decode_column(target_encoded, target_enc, result[target_col].dtype)\n",
    "    result[target_col] = restored_col\n",
    "\n",
    "    # помечаем восстановленные ячейки\n",
    "    if restored_mask is not None:\n",
    "        restored_mask.loc[mask_unknown, target_col] = True\n",
    "\n",
    "    print(\n",
    "        f\"'{target_col}': линейная регрессия восстановила \"\n",
    "        f\"{int(mask_unknown.sum())} пропусков\"\n",
    "    )\n",
    "\n",
    "    return result, restored_mask\n",
    "\n",
    "def restore_dataset_full_regression(damaged_df: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Восстанавливает ВСЕ столбцы damaged_df с помощью линейной регрессии,\n",
    "    НЕ используя исходный (неповреждённый) датасет.\n",
    "    Возвращает:\n",
    "      - restored: восстановленный датасет\n",
    "      - restored_mask: DataFrame того же размера с True в восстановленных ячейках\n",
    "    \"\"\"\n",
    "    restored = damaged_df.copy()\n",
    "\n",
    "    # маска восстановленных ячеек\n",
    "    restored_mask = pd.DataFrame(False, index=restored.index, columns=restored.columns)\n",
    "\n",
    "    encoders = {}\n",
    "    for col in restored.columns:\n",
    "        encoders[col] = build_column_encoder(restored[col])\n",
    "\n",
    "    for col in restored.columns:\n",
    "        if restored[col].isna().sum() == 0:\n",
    "            continue\n",
    "        restored, restored_mask = impute_column_by_regression(restored, col, encoders, restored_mask)\n",
    "\n",
    "    return restored, restored_mask\n",
    "\n",
    "def show_linear_regression_weights(df: pd.DataFrame, target_col: str):\n",
    "    \"\"\"\n",
    "    Обучает линейную регрессию для target_col на текущем df\n",
    "    и печатает коэффициенты w_n по признакам.\n",
    "    \"\"\"\n",
    "    if target_col not in df.columns:\n",
    "        raise KeyError(f\"Колонка '{target_col}' не найдена\")\n",
    "\n",
    "    # строим энкодеры\n",
    "    encoders = {col: build_column_encoder(df[col]) for col in df.columns}\n",
    "\n",
    "    target_enc = encoders[target_col]\n",
    "    y = encode_column(df[target_col], target_enc)\n",
    "\n",
    "    feature_cols = [c for c in df.columns if c != target_col]\n",
    "    X = pd.DataFrame(index=df.index)\n",
    "    for col in feature_cols:\n",
    "        X[col] = encode_column(df[col], encoders[col])\n",
    "\n",
    "    mask = y.notna()\n",
    "    X_train = X.loc[mask, :].fillna(0)\n",
    "    y_train = y.loc[mask]\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train.values, y_train.values)\n",
    "\n",
    "    print(f\"Целевая колонка: {target_col}\")\n",
    "    print(f\"Свободный член w0: {model.intercept_}\")\n",
    "\n",
    "    # print(\"Коэффициенты w_n по признакам:\")\n",
    "    # for fname, w in zip(feature_cols, model.coef_):\n",
    "    #     print(f\"  {fname:30s} -> {w}\")\n",
    "\n",
    "    print(\"Коэффициенты w_n по признакам:\")\n",
    "    coef_items = list(zip(feature_cols, model.coef_))\n",
    "    # сортировка по модулю веса по убыванию:\n",
    "    coef_items_sorted = sorted(coef_items, key=lambda x: abs(x[1]), reverse=True)\n",
    "    for fname, w in coef_items_sorted:\n",
    "        print(f\"  {fname:30s} -> {w}\")\n",
    "    return coef_items\n",
    "\n",
    "restored_with_linear_regression, restored_lr_mask = restore_dataset_full_regression(dataset_loss_5)\n",
    "# save_current_state(restored_with_linear_regression, \"dataset_loss_5_recovered_with_linear_regression.xlsx\")\n",
    "# save_current_state(restored_lr_mask.astype(int), \"dataset_loss_5_recovered_with_linear_regression_mask.xlsx\")\n",
    "\n",
    "print(show_linear_regression_weights(restored_with_linear_regression, \"Стоимость анализов\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260a29ad",
   "metadata": {},
   "source": [
    "### Calсucate prediction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "81f1f36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean^2 distance (mean):        38304270000.0\n",
      "Euclidean^2 distance (lin. reg.):  37660056932.18776\n",
      "Ratio (mean / lin.reg): 1.017106003556294\n",
      "Chebyshev distance (mean):         28400.0\n",
      "Chebyshev distance (lin. reg.):   28601.466421864603\n",
      "Ratio Chebyshev (mean / lin.reg): 0.9929560806815628\n"
     ]
    }
   ],
   "source": [
    "def calculate_eucluidian_dist(df1: pd.DataFrame, df2: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Считает суммарное квадрат Евклидово расстояние между df1 и df2\n",
    "    по числовым столбцам. Чем МЕНЬШЕ значение, тем ближе восстановленный\n",
    "    датасет к исходному.\n",
    "    \"\"\"\n",
    "    # приведение к одинаковому порядку столбцов и индексов\n",
    "    df2 = df2.reindex(index=df1.index, columns=df1.columns)\n",
    "\n",
    "    # берём только числовые столбцы (для строк будет отдельная метрика, если нужно)\n",
    "    num1 = df1.select_dtypes(include=[np.number])\n",
    "    num2 = df2[num1.columns]\n",
    "\n",
    "    # заменим NaN нулями, чтобы не ломать расчёт\n",
    "    num1 = num1.fillna(0)\n",
    "    num2 = num2.fillna(0)\n",
    "\n",
    "    diff = num1.values - num2.values\n",
    "    sq_dist = np.sum(diff ** 2)\n",
    "\n",
    "    return float(sq_dist)\n",
    "\n",
    "def calculate_chebyshev_dist(df1: pd.DataFrame, df2: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Считает расстояние Чебышева между df1 и df2\n",
    "    по числовым столбцам: max |x_i - y_i|.\n",
    "    Чем МЕНЬШЕ значение, тем ближе восстановленный датасет к исходному.\n",
    "    \"\"\"\n",
    "    df2 = df2.reindex(index=df1.index, columns=df1.columns)\n",
    "\n",
    "    num1 = df1.select_dtypes(include=[np.number])\n",
    "    num2 = df2[num1.columns]\n",
    "\n",
    "    num1 = num1.fillna(0)\n",
    "    num2 = num2.fillna(0)\n",
    "\n",
    "    diff = np.abs(num1.values - num2.values)\n",
    "    cheb_dist = np.max(diff)\n",
    "\n",
    "    return float(cheb_dist)\n",
    "\n",
    "# Евклидово расстояние\n",
    "dist_mean_eucl = calculate_eucluidian_dist(dataset, restored_with_mean_dataset)\n",
    "dist_lr_eucl   = calculate_eucluidian_dist(dataset, restored_with_linear_regression)\n",
    "\n",
    "print(f\"Euclidean^2 distance (mean):        {dist_mean_eucl}\")\n",
    "print(f\"Euclidean^2 distance (lin. reg.):  {dist_lr_eucl}\")\n",
    "print(\"Ratio (mean / lin.reg):\", dist_mean_eucl / dist_lr_eucl)\n",
    "\n",
    "# Расстояние Чебышева\n",
    "dist_mean_cheb = calculate_chebyshev_dist(dataset, restored_with_mean_dataset)\n",
    "dist_lr_cheb   = calculate_chebyshev_dist(dataset, restored_with_linear_regression)\n",
    "\n",
    "print(f\"Chebyshev distance (mean):         {dist_mean_cheb}\")\n",
    "print(f\"Chebyshev distance (lin. reg.):   {dist_lr_cheb}\")\n",
    "print(\"Ratio Chebyshev (mean / lin.reg):\", dist_mean_cheb / dist_lr_cheb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
